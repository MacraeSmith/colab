{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MacraeSmith/helloAI/blob/main/Assignment7_metaltest1_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CzeLeqEaNj8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25fc217-991e-4536-a25c-26997c75207a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16028126.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 272175.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5031954.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 3901748.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define linear model we will use below\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# modified from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "# get our competing devices ready ... go ahead and init all three here, but ONLY USE ONE during each test\n",
        "# scroll down below and replace the references to gpu_, tpu_, cpu with whichever device you are testing\n",
        "# make sure you replace ALL of them\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "tpu_device = torch.device(\"xla\")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# first hyperparam\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# Show some sample data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# sample code for working with m1\n",
        "  # Create a Tensor directly on the mps device\n",
        "  #x = torch.ones(5, device=mps_device)\n",
        "  # Or\n",
        "  #x = torch.ones(5, device=\"mps\")\n",
        "  # Any operation happens on the GPU\n",
        "  #y = x * 2\n",
        "\n",
        "  # Move your model to mps just like any other device\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CPU\n",
        "model = NeuralNetwork().to(cpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using CPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, cpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using CPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, cpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "rGBLpb2jGRtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382fcf61-3af5-49e8-f578-d5ce8833e15b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting timer for training using CPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.310128  [   64/60000]\n",
            "loss: 2.298631  [ 6464/60000]\n",
            "loss: 2.272886  [12864/60000]\n",
            "loss: 2.262022  [19264/60000]\n",
            "loss: 2.262884  [25664/60000]\n",
            "loss: 2.224250  [32064/60000]\n",
            "loss: 2.230855  [38464/60000]\n",
            "loss: 2.206067  [44864/60000]\n",
            "loss: 2.202664  [51264/60000]\n",
            "loss: 2.159739  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.180223  [   64/60000]\n",
            "loss: 2.167925  [ 6464/60000]\n",
            "loss: 2.111321  [12864/60000]\n",
            "loss: 2.120037  [19264/60000]\n",
            "loss: 2.083220  [25664/60000]\n",
            "loss: 2.016685  [32064/60000]\n",
            "loss: 2.037050  [38464/60000]\n",
            "loss: 1.970124  [44864/60000]\n",
            "loss: 1.972563  [51264/60000]\n",
            "loss: 1.886436  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.934161  [   64/60000]\n",
            "loss: 1.902970  [ 6464/60000]\n",
            "loss: 1.785077  [12864/60000]\n",
            "loss: 1.818120  [19264/60000]\n",
            "loss: 1.720476  [25664/60000]\n",
            "loss: 1.655905  [32064/60000]\n",
            "loss: 1.670609  [38464/60000]\n",
            "loss: 1.579665  [44864/60000]\n",
            "loss: 1.605715  [51264/60000]\n",
            "loss: 1.486297  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.585816  [   64/60000]\n",
            "loss: 1.549551  [ 6464/60000]\n",
            "loss: 1.394688  [12864/60000]\n",
            "loss: 1.465217  [19264/60000]\n",
            "loss: 1.359055  [25664/60000]\n",
            "loss: 1.336366  [32064/60000]\n",
            "loss: 1.352614  [38464/60000]\n",
            "loss: 1.279068  [44864/60000]\n",
            "loss: 1.318557  [51264/60000]\n",
            "loss: 1.215528  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.323319  [   64/60000]\n",
            "loss: 1.305859  [ 6464/60000]\n",
            "loss: 1.133143  [12864/60000]\n",
            "loss: 1.241029  [19264/60000]\n",
            "loss: 1.130355  [25664/60000]\n",
            "loss: 1.137294  [32064/60000]\n",
            "loss: 1.162642  [38464/60000]\n",
            "loss: 1.097920  [44864/60000]\n",
            "loss: 1.142102  [51264/60000]\n",
            "loss: 1.061363  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.151575  [   64/60000]\n",
            "loss: 1.157085  [ 6464/60000]\n",
            "loss: 0.966789  [12864/60000]\n",
            "loss: 1.103901  [19264/60000]\n",
            "loss: 0.990261  [25664/60000]\n",
            "loss: 1.006068  [32064/60000]\n",
            "loss: 1.044713  [38464/60000]\n",
            "loss: 0.983934  [44864/60000]\n",
            "loss: 1.027680  [51264/60000]\n",
            "loss: 0.965981  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.033022  [   64/60000]\n",
            "loss: 1.061782  [ 6464/60000]\n",
            "loss: 0.854809  [12864/60000]\n",
            "loss: 1.013287  [19264/60000]\n",
            "loss: 0.901401  [25664/60000]\n",
            "loss: 0.914787  [32064/60000]\n",
            "loss: 0.966817  [38464/60000]\n",
            "loss: 0.910309  [44864/60000]\n",
            "loss: 0.948894  [51264/60000]\n",
            "loss: 0.902386  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.946746  [   64/60000]\n",
            "loss: 0.996397  [ 6464/60000]\n",
            "loss: 0.775784  [12864/60000]\n",
            "loss: 0.949454  [19264/60000]\n",
            "loss: 0.841567  [25664/60000]\n",
            "loss: 0.848067  [32064/60000]\n",
            "loss: 0.911804  [38464/60000]\n",
            "loss: 0.861511  [44864/60000]\n",
            "loss: 0.892684  [51264/60000]\n",
            "loss: 0.856408  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.880926  [   64/60000]\n",
            "loss: 0.947565  [ 6464/60000]\n",
            "loss: 0.716797  [12864/60000]\n",
            "loss: 0.901751  [19264/60000]\n",
            "loss: 0.798389  [25664/60000]\n",
            "loss: 0.797603  [32064/60000]\n",
            "loss: 0.869764  [38464/60000]\n",
            "loss: 0.827259  [44864/60000]\n",
            "loss: 0.850719  [51264/60000]\n",
            "loss: 0.820752  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.828051  [   64/60000]\n",
            "loss: 0.908058  [ 6464/60000]\n",
            "loss: 0.670481  [12864/60000]\n",
            "loss: 0.864679  [19264/60000]\n",
            "loss: 0.765105  [25664/60000]\n",
            "loss: 0.758645  [32064/60000]\n",
            "loss: 0.835604  [38464/60000]\n",
            "loss: 0.801990  [44864/60000]\n",
            "loss: 0.817996  [51264/60000]\n",
            "loss: 0.791438  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.784206  [   64/60000]\n",
            "loss: 0.874201  [ 6464/60000]\n",
            "loss: 0.632831  [12864/60000]\n",
            "loss: 0.834962  [19264/60000]\n",
            "loss: 0.738306  [25664/60000]\n",
            "loss: 0.727678  [32064/60000]\n",
            "loss: 0.806502  [38464/60000]\n",
            "loss: 0.782130  [44864/60000]\n",
            "loss: 0.791618  [51264/60000]\n",
            "loss: 0.766372  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.746704  [   64/60000]\n",
            "loss: 0.844021  [ 6464/60000]\n",
            "loss: 0.601302  [12864/60000]\n",
            "loss: 0.810588  [19264/60000]\n",
            "loss: 0.715925  [25664/60000]\n",
            "loss: 0.702673  [32064/60000]\n",
            "loss: 0.780731  [38464/60000]\n",
            "loss: 0.765563  [44864/60000]\n",
            "loss: 0.769527  [51264/60000]\n",
            "loss: 0.744084  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.713960  [   64/60000]\n",
            "loss: 0.816547  [ 6464/60000]\n",
            "loss: 0.574246  [12864/60000]\n",
            "loss: 0.790001  [19264/60000]\n",
            "loss: 0.696766  [25664/60000]\n",
            "loss: 0.682075  [32064/60000]\n",
            "loss: 0.757249  [38464/60000]\n",
            "loss: 0.751233  [44864/60000]\n",
            "loss: 0.750496  [51264/60000]\n",
            "loss: 0.723767  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.685028  [   64/60000]\n",
            "loss: 0.791169  [ 6464/60000]\n",
            "loss: 0.550782  [12864/60000]\n",
            "loss: 0.772257  [19264/60000]\n",
            "loss: 0.680059  [25664/60000]\n",
            "loss: 0.664676  [32064/60000]\n",
            "loss: 0.735599  [38464/60000]\n",
            "loss: 0.738534  [44864/60000]\n",
            "loss: 0.733912  [51264/60000]\n",
            "loss: 0.705081  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.659277  [   64/60000]\n",
            "loss: 0.767769  [ 6464/60000]\n",
            "loss: 0.530261  [12864/60000]\n",
            "loss: 0.756526  [19264/60000]\n",
            "loss: 0.665488  [25664/60000]\n",
            "loss: 0.649924  [32064/60000]\n",
            "loss: 0.715461  [38464/60000]\n",
            "loss: 0.727127  [44864/60000]\n",
            "loss: 0.719355  [51264/60000]\n",
            "loss: 0.687759  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.636209  [   64/60000]\n",
            "loss: 0.746271  [ 6464/60000]\n",
            "loss: 0.512193  [12864/60000]\n",
            "loss: 0.742269  [19264/60000]\n",
            "loss: 0.652701  [25664/60000]\n",
            "loss: 0.637387  [32064/60000]\n",
            "loss: 0.696641  [38464/60000]\n",
            "loss: 0.716775  [44864/60000]\n",
            "loss: 0.706574  [51264/60000]\n",
            "loss: 0.671480  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.615457  [   64/60000]\n",
            "loss: 0.726399  [ 6464/60000]\n",
            "loss: 0.496077  [12864/60000]\n",
            "loss: 0.729301  [19264/60000]\n",
            "loss: 0.641353  [25664/60000]\n",
            "loss: 0.626566  [32064/60000]\n",
            "loss: 0.679197  [38464/60000]\n",
            "loss: 0.707732  [44864/60000]\n",
            "loss: 0.695433  [51264/60000]\n",
            "loss: 0.656348  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.596744  [   64/60000]\n",
            "loss: 0.708078  [ 6464/60000]\n",
            "loss: 0.481660  [12864/60000]\n",
            "loss: 0.717437  [19264/60000]\n",
            "loss: 0.631250  [25664/60000]\n",
            "loss: 0.617043  [32064/60000]\n",
            "loss: 0.663083  [38464/60000]\n",
            "loss: 0.699944  [44864/60000]\n",
            "loss: 0.685667  [51264/60000]\n",
            "loss: 0.642179  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.579765  [   64/60000]\n",
            "loss: 0.691213  [ 6464/60000]\n",
            "loss: 0.468758  [12864/60000]\n",
            "loss: 0.706470  [19264/60000]\n",
            "loss: 0.622330  [25664/60000]\n",
            "loss: 0.608688  [32064/60000]\n",
            "loss: 0.648224  [38464/60000]\n",
            "loss: 0.693338  [44864/60000]\n",
            "loss: 0.677268  [51264/60000]\n",
            "loss: 0.628926  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.564344  [   64/60000]\n",
            "loss: 0.675778  [ 6464/60000]\n",
            "loss: 0.457153  [12864/60000]\n",
            "loss: 0.696221  [19264/60000]\n",
            "loss: 0.614395  [25664/60000]\n",
            "loss: 0.601329  [32064/60000]\n",
            "loss: 0.634575  [38464/60000]\n",
            "loss: 0.687874  [44864/60000]\n",
            "loss: 0.670155  [51264/60000]\n",
            "loss: 0.616432  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.550324  [   64/60000]\n",
            "loss: 0.661698  [ 6464/60000]\n",
            "loss: 0.446676  [12864/60000]\n",
            "loss: 0.686636  [19264/60000]\n",
            "loss: 0.607225  [25664/60000]\n",
            "loss: 0.594824  [32064/60000]\n",
            "loss: 0.621933  [38464/60000]\n",
            "loss: 0.683451  [44864/60000]\n",
            "loss: 0.664135  [51264/60000]\n",
            "loss: 0.604612  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.537538  [   64/60000]\n",
            "loss: 0.648824  [ 6464/60000]\n",
            "loss: 0.437062  [12864/60000]\n",
            "loss: 0.677602  [19264/60000]\n",
            "loss: 0.600648  [25664/60000]\n",
            "loss: 0.589022  [32064/60000]\n",
            "loss: 0.610323  [38464/60000]\n",
            "loss: 0.679994  [44864/60000]\n",
            "loss: 0.658990  [51264/60000]\n",
            "loss: 0.593500  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.525819  [   64/60000]\n",
            "loss: 0.637017  [ 6464/60000]\n",
            "loss: 0.428286  [12864/60000]\n",
            "loss: 0.669050  [19264/60000]\n",
            "loss: 0.594433  [25664/60000]\n",
            "loss: 0.583742  [32064/60000]\n",
            "loss: 0.599651  [38464/60000]\n",
            "loss: 0.677310  [44864/60000]\n",
            "loss: 0.654659  [51264/60000]\n",
            "loss: 0.582974  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.514941  [   64/60000]\n",
            "loss: 0.626193  [ 6464/60000]\n",
            "loss: 0.420186  [12864/60000]\n",
            "loss: 0.660847  [19264/60000]\n",
            "loss: 0.588624  [25664/60000]\n",
            "loss: 0.578842  [32064/60000]\n",
            "loss: 0.589818  [38464/60000]\n",
            "loss: 0.675334  [44864/60000]\n",
            "loss: 0.650910  [51264/60000]\n",
            "loss: 0.572890  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.504777  [   64/60000]\n",
            "loss: 0.616268  [ 6464/60000]\n",
            "loss: 0.412689  [12864/60000]\n",
            "loss: 0.653018  [19264/60000]\n",
            "loss: 0.583001  [25664/60000]\n",
            "loss: 0.574222  [32064/60000]\n",
            "loss: 0.580738  [38464/60000]\n",
            "loss: 0.674044  [44864/60000]\n",
            "loss: 0.647637  [51264/60000]\n",
            "loss: 0.563231  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.495234  [   64/60000]\n",
            "loss: 0.607110  [ 6464/60000]\n",
            "loss: 0.405716  [12864/60000]\n",
            "loss: 0.645558  [19264/60000]\n",
            "loss: 0.577515  [25664/60000]\n",
            "loss: 0.569765  [32064/60000]\n",
            "loss: 0.572409  [38464/60000]\n",
            "loss: 0.673248  [44864/60000]\n",
            "loss: 0.644782  [51264/60000]\n",
            "loss: 0.553970  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.486269  [   64/60000]\n",
            "loss: 0.598670  [ 6464/60000]\n",
            "loss: 0.399257  [12864/60000]\n",
            "loss: 0.638425  [19264/60000]\n",
            "loss: 0.572156  [25664/60000]\n",
            "loss: 0.565431  [32064/60000]\n",
            "loss: 0.564744  [38464/60000]\n",
            "loss: 0.672865  [44864/60000]\n",
            "loss: 0.642250  [51264/60000]\n",
            "loss: 0.545017  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.477788  [   64/60000]\n",
            "loss: 0.590877  [ 6464/60000]\n",
            "loss: 0.393190  [12864/60000]\n",
            "loss: 0.631633  [19264/60000]\n",
            "loss: 0.566873  [25664/60000]\n",
            "loss: 0.561235  [32064/60000]\n",
            "loss: 0.557673  [38464/60000]\n",
            "loss: 0.672802  [44864/60000]\n",
            "loss: 0.639865  [51264/60000]\n",
            "loss: 0.536389  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.469763  [   64/60000]\n",
            "loss: 0.583686  [ 6464/60000]\n",
            "loss: 0.387489  [12864/60000]\n",
            "loss: 0.625164  [19264/60000]\n",
            "loss: 0.561650  [25664/60000]\n",
            "loss: 0.557098  [32064/60000]\n",
            "loss: 0.551112  [38464/60000]\n",
            "loss: 0.672947  [44864/60000]\n",
            "loss: 0.637664  [51264/60000]\n",
            "loss: 0.528032  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.462162  [   64/60000]\n",
            "loss: 0.576995  [ 6464/60000]\n",
            "loss: 0.382100  [12864/60000]\n",
            "loss: 0.618941  [19264/60000]\n",
            "loss: 0.556518  [25664/60000]\n",
            "loss: 0.552982  [32064/60000]\n",
            "loss: 0.545020  [38464/60000]\n",
            "loss: 0.673299  [44864/60000]\n",
            "loss: 0.635644  [51264/60000]\n",
            "loss: 0.519903  [57664/60000]\n",
            "completed training in ... 363.3326156139374s\n",
            "starting timer for testing using CPU...\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.545879 \n",
            "\n",
            "completed testing in ... 1.4610469341278076s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#GPU\n",
        "model = NeuralNetwork().to(gpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using GPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, gpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using GPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, gpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0eDaN0QGT69",
        "outputId": "d18ee903-a96a-4e41-a6c7-c79ce71c49c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting timer for training using GPU...\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.314056  [   64/60000]\n",
            "loss: 2.297122  [ 6464/60000]\n",
            "loss: 2.279072  [12864/60000]\n",
            "loss: 2.267997  [19264/60000]\n",
            "loss: 2.260455  [25664/60000]\n",
            "loss: 2.232008  [32064/60000]\n",
            "loss: 2.228499  [38464/60000]\n",
            "loss: 2.206622  [44864/60000]\n",
            "loss: 2.200884  [51264/60000]\n",
            "loss: 2.163253  [57664/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.176191  [   64/60000]\n",
            "loss: 2.165034  [ 6464/60000]\n",
            "loss: 2.106558  [12864/60000]\n",
            "loss: 2.126788  [19264/60000]\n",
            "loss: 2.083439  [25664/60000]\n",
            "loss: 2.017687  [32064/60000]\n",
            "loss: 2.044446  [38464/60000]\n",
            "loss: 1.968592  [44864/60000]\n",
            "loss: 1.975660  [51264/60000]\n",
            "loss: 1.902683  [57664/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.926393  [   64/60000]\n",
            "loss: 1.899976  [ 6464/60000]\n",
            "loss: 1.779862  [12864/60000]\n",
            "loss: 1.839132  [19264/60000]\n",
            "loss: 1.718462  [25664/60000]\n",
            "loss: 1.658816  [32064/60000]\n",
            "loss: 1.688530  [38464/60000]\n",
            "loss: 1.580364  [44864/60000]\n",
            "loss: 1.612754  [51264/60000]\n",
            "loss: 1.511947  [57664/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.579338  [   64/60000]\n",
            "loss: 1.548512  [ 6464/60000]\n",
            "loss: 1.397013  [12864/60000]\n",
            "loss: 1.489207  [19264/60000]\n",
            "loss: 1.356481  [25664/60000]\n",
            "loss: 1.347211  [32064/60000]\n",
            "loss: 1.365291  [38464/60000]\n",
            "loss: 1.278861  [44864/60000]\n",
            "loss: 1.323179  [51264/60000]\n",
            "loss: 1.230656  [57664/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.319233  [   64/60000]\n",
            "loss: 1.301854  [ 6464/60000]\n",
            "loss: 1.136236  [12864/60000]\n",
            "loss: 1.258769  [19264/60000]\n",
            "loss: 1.129574  [25664/60000]\n",
            "loss: 1.148790  [32064/60000]\n",
            "loss: 1.171502  [38464/60000]\n",
            "loss: 1.096181  [44864/60000]\n",
            "loss: 1.146818  [51264/60000]\n",
            "loss: 1.072047  [57664/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.149166  [   64/60000]\n",
            "loss: 1.149968  [ 6464/60000]\n",
            "loss: 0.969214  [12864/60000]\n",
            "loss: 1.117824  [19264/60000]\n",
            "loss: 0.993278  [25664/60000]\n",
            "loss: 1.017045  [32064/60000]\n",
            "loss: 1.054331  [38464/60000]\n",
            "loss: 0.981556  [44864/60000]\n",
            "loss: 1.033590  [51264/60000]\n",
            "loss: 0.974930  [57664/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.032344  [   64/60000]\n",
            "loss: 1.052803  [ 6464/60000]\n",
            "loss: 0.856079  [12864/60000]\n",
            "loss: 1.024870  [19264/60000]\n",
            "loss: 0.907015  [25664/60000]\n",
            "loss: 0.924223  [32064/60000]\n",
            "loss: 0.978275  [38464/60000]\n",
            "loss: 0.907303  [44864/60000]\n",
            "loss: 0.955908  [51264/60000]\n",
            "loss: 0.909940  [57664/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.946809  [   64/60000]\n",
            "loss: 0.985811  [ 6464/60000]\n",
            "loss: 0.774843  [12864/60000]\n",
            "loss: 0.959511  [19264/60000]\n",
            "loss: 0.848696  [25664/60000]\n",
            "loss: 0.855899  [32064/60000]\n",
            "loss: 0.925054  [38464/60000]\n",
            "loss: 0.857733  [44864/60000]\n",
            "loss: 0.900220  [51264/60000]\n",
            "loss: 0.863261  [57664/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.881254  [   64/60000]\n",
            "loss: 0.935465  [ 6464/60000]\n",
            "loss: 0.714038  [12864/60000]\n",
            "loss: 0.911023  [19264/60000]\n",
            "loss: 0.806621  [25664/60000]\n",
            "loss: 0.804453  [32064/60000]\n",
            "loss: 0.884737  [38464/60000]\n",
            "loss: 0.823197  [44864/60000]\n",
            "loss: 0.858528  [51264/60000]\n",
            "loss: 0.827314  [57664/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.828838  [   64/60000]\n",
            "loss: 0.895016  [ 6464/60000]\n",
            "loss: 0.666574  [12864/60000]\n",
            "loss: 0.873519  [19264/60000]\n",
            "loss: 0.774245  [25664/60000]\n",
            "loss: 0.764700  [32064/60000]\n",
            "loss: 0.851862  [38464/60000]\n",
            "loss: 0.797679  [44864/60000]\n",
            "loss: 0.825842  [51264/60000]\n",
            "loss: 0.798194  [57664/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.785343  [   64/60000]\n",
            "loss: 0.860573  [ 6464/60000]\n",
            "loss: 0.628190  [12864/60000]\n",
            "loss: 0.843428  [19264/60000]\n",
            "loss: 0.747889  [25664/60000]\n",
            "loss: 0.733170  [32064/60000]\n",
            "loss: 0.823531  [38464/60000]\n",
            "loss: 0.777591  [44864/60000]\n",
            "loss: 0.799261  [51264/60000]\n",
            "loss: 0.773725  [57664/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.748308  [   64/60000]\n",
            "loss: 0.830078  [ 6464/60000]\n",
            "loss: 0.596232  [12864/60000]\n",
            "loss: 0.818638  [19264/60000]\n",
            "loss: 0.725606  [25664/60000]\n",
            "loss: 0.707558  [32064/60000]\n",
            "loss: 0.798125  [38464/60000]\n",
            "loss: 0.760823  [44864/60000]\n",
            "loss: 0.776860  [51264/60000]\n",
            "loss: 0.752410  [57664/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.716103  [   64/60000]\n",
            "loss: 0.802527  [ 6464/60000]\n",
            "loss: 0.569085  [12864/60000]\n",
            "loss: 0.797579  [19264/60000]\n",
            "loss: 0.706350  [25664/60000]\n",
            "loss: 0.686386  [32064/60000]\n",
            "loss: 0.774888  [38464/60000]\n",
            "loss: 0.746157  [44864/60000]\n",
            "loss: 0.757544  [51264/60000]\n",
            "loss: 0.733220  [57664/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.687838  [   64/60000]\n",
            "loss: 0.777358  [ 6464/60000]\n",
            "loss: 0.545746  [12864/60000]\n",
            "loss: 0.779133  [19264/60000]\n",
            "loss: 0.689523  [25664/60000]\n",
            "loss: 0.668634  [32064/60000]\n",
            "loss: 0.753426  [38464/60000]\n",
            "loss: 0.733205  [44864/60000]\n",
            "loss: 0.740575  [51264/60000]\n",
            "loss: 0.715826  [57664/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.662719  [   64/60000]\n",
            "loss: 0.754175  [ 6464/60000]\n",
            "loss: 0.525311  [12864/60000]\n",
            "loss: 0.762602  [19264/60000]\n",
            "loss: 0.674659  [25664/60000]\n",
            "loss: 0.653529  [32064/60000]\n",
            "loss: 0.733476  [38464/60000]\n",
            "loss: 0.721559  [44864/60000]\n",
            "loss: 0.725504  [51264/60000]\n",
            "loss: 0.699899  [57664/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.640328  [   64/60000]\n",
            "loss: 0.732836  [ 6464/60000]\n",
            "loss: 0.507304  [12864/60000]\n",
            "loss: 0.747648  [19264/60000]\n",
            "loss: 0.661542  [25664/60000]\n",
            "loss: 0.640427  [32064/60000]\n",
            "loss: 0.714892  [38464/60000]\n",
            "loss: 0.711209  [44864/60000]\n",
            "loss: 0.712132  [51264/60000]\n",
            "loss: 0.685175  [57664/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.620337  [   64/60000]\n",
            "loss: 0.713230  [ 6464/60000]\n",
            "loss: 0.491339  [12864/60000]\n",
            "loss: 0.733924  [19264/60000]\n",
            "loss: 0.649965  [25664/60000]\n",
            "loss: 0.629107  [32064/60000]\n",
            "loss: 0.697518  [38464/60000]\n",
            "loss: 0.702091  [44864/60000]\n",
            "loss: 0.700277  [51264/60000]\n",
            "loss: 0.671633  [57664/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.602328  [   64/60000]\n",
            "loss: 0.695205  [ 6464/60000]\n",
            "loss: 0.477083  [12864/60000]\n",
            "loss: 0.721186  [19264/60000]\n",
            "loss: 0.639579  [25664/60000]\n",
            "loss: 0.619287  [32064/60000]\n",
            "loss: 0.681417  [38464/60000]\n",
            "loss: 0.694070  [44864/60000]\n",
            "loss: 0.690043  [51264/60000]\n",
            "loss: 0.659112  [57664/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.585969  [   64/60000]\n",
            "loss: 0.678821  [ 6464/60000]\n",
            "loss: 0.464315  [12864/60000]\n",
            "loss: 0.709391  [19264/60000]\n",
            "loss: 0.630281  [25664/60000]\n",
            "loss: 0.610732  [32064/60000]\n",
            "loss: 0.666449  [38464/60000]\n",
            "loss: 0.686959  [44864/60000]\n",
            "loss: 0.681056  [51264/60000]\n",
            "loss: 0.647382  [57664/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.571050  [   64/60000]\n",
            "loss: 0.663832  [ 6464/60000]\n",
            "loss: 0.452697  [12864/60000]\n",
            "loss: 0.698302  [19264/60000]\n",
            "loss: 0.621952  [25664/60000]\n",
            "loss: 0.603212  [32064/60000]\n",
            "loss: 0.652626  [38464/60000]\n",
            "loss: 0.680855  [44864/60000]\n",
            "loss: 0.673233  [51264/60000]\n",
            "loss: 0.636358  [57664/60000]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.557477  [   64/60000]\n",
            "loss: 0.650057  [ 6464/60000]\n",
            "loss: 0.442147  [12864/60000]\n",
            "loss: 0.687857  [19264/60000]\n",
            "loss: 0.614333  [25664/60000]\n",
            "loss: 0.596454  [32064/60000]\n",
            "loss: 0.639794  [38464/60000]\n",
            "loss: 0.675674  [44864/60000]\n",
            "loss: 0.666531  [51264/60000]\n",
            "loss: 0.625929  [57664/60000]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.544868  [   64/60000]\n",
            "loss: 0.637315  [ 6464/60000]\n",
            "loss: 0.432564  [12864/60000]\n",
            "loss: 0.678152  [19264/60000]\n",
            "loss: 0.607255  [25664/60000]\n",
            "loss: 0.590282  [32064/60000]\n",
            "loss: 0.627892  [38464/60000]\n",
            "loss: 0.671377  [44864/60000]\n",
            "loss: 0.660662  [51264/60000]\n",
            "loss: 0.616014  [57664/60000]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.533169  [   64/60000]\n",
            "loss: 0.625533  [ 6464/60000]\n",
            "loss: 0.423759  [12864/60000]\n",
            "loss: 0.668944  [19264/60000]\n",
            "loss: 0.600624  [25664/60000]\n",
            "loss: 0.584596  [32064/60000]\n",
            "loss: 0.616852  [38464/60000]\n",
            "loss: 0.667811  [44864/60000]\n",
            "loss: 0.655543  [51264/60000]\n",
            "loss: 0.606526  [57664/60000]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.522224  [   64/60000]\n",
            "loss: 0.614679  [ 6464/60000]\n",
            "loss: 0.415603  [12864/60000]\n",
            "loss: 0.660191  [19264/60000]\n",
            "loss: 0.594265  [25664/60000]\n",
            "loss: 0.579207  [32064/60000]\n",
            "loss: 0.606652  [38464/60000]\n",
            "loss: 0.664933  [44864/60000]\n",
            "loss: 0.651125  [51264/60000]\n",
            "loss: 0.597432  [57664/60000]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.511948  [   64/60000]\n",
            "loss: 0.604662  [ 6464/60000]\n",
            "loss: 0.408051  [12864/60000]\n",
            "loss: 0.651844  [19264/60000]\n",
            "loss: 0.588113  [25664/60000]\n",
            "loss: 0.574101  [32064/60000]\n",
            "loss: 0.597232  [38464/60000]\n",
            "loss: 0.662639  [44864/60000]\n",
            "loss: 0.647245  [51264/60000]\n",
            "loss: 0.588671  [57664/60000]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.502249  [   64/60000]\n",
            "loss: 0.595351  [ 6464/60000]\n",
            "loss: 0.401042  [12864/60000]\n",
            "loss: 0.643887  [19264/60000]\n",
            "loss: 0.582128  [25664/60000]\n",
            "loss: 0.569174  [32064/60000]\n",
            "loss: 0.588481  [38464/60000]\n",
            "loss: 0.660908  [44864/60000]\n",
            "loss: 0.643834  [51264/60000]\n",
            "loss: 0.580154  [57664/60000]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.493082  [   64/60000]\n",
            "loss: 0.586669  [ 6464/60000]\n",
            "loss: 0.394488  [12864/60000]\n",
            "loss: 0.636270  [19264/60000]\n",
            "loss: 0.576292  [25664/60000]\n",
            "loss: 0.564400  [32064/60000]\n",
            "loss: 0.580324  [38464/60000]\n",
            "loss: 0.659598  [44864/60000]\n",
            "loss: 0.640794  [51264/60000]\n",
            "loss: 0.571883  [57664/60000]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.484321  [   64/60000]\n",
            "loss: 0.578606  [ 6464/60000]\n",
            "loss: 0.388434  [12864/60000]\n",
            "loss: 0.628982  [19264/60000]\n",
            "loss: 0.570527  [25664/60000]\n",
            "loss: 0.559705  [32064/60000]\n",
            "loss: 0.572738  [38464/60000]\n",
            "loss: 0.658697  [44864/60000]\n",
            "loss: 0.638114  [51264/60000]\n",
            "loss: 0.563825  [57664/60000]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.475970  [   64/60000]\n",
            "loss: 0.571146  [ 6464/60000]\n",
            "loss: 0.382783  [12864/60000]\n",
            "loss: 0.621968  [19264/60000]\n",
            "loss: 0.564836  [25664/60000]\n",
            "loss: 0.555004  [32064/60000]\n",
            "loss: 0.565661  [38464/60000]\n",
            "loss: 0.658141  [44864/60000]\n",
            "loss: 0.635635  [51264/60000]\n",
            "loss: 0.555972  [57664/60000]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.468069  [   64/60000]\n",
            "loss: 0.564226  [ 6464/60000]\n",
            "loss: 0.377506  [12864/60000]\n",
            "loss: 0.615253  [19264/60000]\n",
            "loss: 0.559242  [25664/60000]\n",
            "loss: 0.550335  [32064/60000]\n",
            "loss: 0.559079  [38464/60000]\n",
            "loss: 0.657897  [44864/60000]\n",
            "loss: 0.633293  [51264/60000]\n",
            "loss: 0.548350  [57664/60000]\n",
            "completed training in ... 268.5908842086792s\n",
            "starting timer for testing using GPU...\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.547530 \n",
            "\n",
            "completed testing in ... 1.3842406272888184s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TPU\n",
        "model = NeuralNetwork().to(tpu_device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"starting timer for training using TPU...\")\n",
        "start = time.time()\n",
        "epochs = 30 \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer, tpu_device)\n",
        "print(f\"completed training in ... {time.time()-start}s\")\n",
        "\n",
        "print(\"starting timer for testing using TPU...\")\n",
        "start = time.time()\n",
        "test(test_dataloader, model, loss_fn, tpu_device)\n",
        "print(f\"completed testing in ... {time.time()-start}s\")"
      ],
      "metadata": {
        "id": "6-ieBXPOGZ-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "1f733254-f14c-43a6-d31d-7cce2fc9d5d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9486a33e2990>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting timer for training using TPU...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for xla devices"
          ]
        }
      ]
    }
  ]
}